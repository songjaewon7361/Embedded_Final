//
//  CameraSessionManager.swift
//  finalApp
//
//  Created by Chaeyeong Park on 2025/06/08.
//

import Foundation
import AVFoundation
import Vision
import Combine            // â† ObservableObject í”„ë¡œí† ì½œ ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€

/// AVCaptureSessionì„ ì„¸íŒ…í•˜ê³ , ë“¤ì–´ì˜¤ëŠ” ê° í”„ë ˆì„ë§ˆë‹¤
/// Core ML ëª¨ë¸ë¡œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ @Published detections ë°°ì—´ì— ë‹´ì•„ì¤ë‹ˆë‹¤.
class CameraSessionManager: NSObject, ObservableObject {
    // MARK: - í¼ë¸”ë¦¬ì‹œí•  íƒì§€ ê²°ê³¼
    @Published var detections: [DetectedObject] = []
    
    /// ìµœì‹  í”„ë ˆì„ì„ CameraViewì—ì„œ OCR ìš©ìœ¼ë¡œ êº¼ë‚´ ì“¸ ìˆ˜ ìˆë„ë¡ ì €ì¥
    var currentBuffer: CVPixelBuffer?
    
    // MARK: - ë‚´ë¶€ ì„¸ì…˜ & ì¶œë ¥
    private let captureSession = AVCaptureSession()
    private let videoOutput = AVCaptureVideoDataOutput()
    
    // VNCoreMLModel ë˜í¼
    private var visionModel: VNCoreMLModel?
    
    override init() {
        super.init()
        // 1) Core ML ëª¨ë¸ ë¡œë“œ
        visionModel = ModelLoader.loadVisionModel()
        // 2) ì¹´ë©”ë¼ ê¶Œí•œ ìš”ì²­ â†’ ìŠ¹ì¸ ì‹œì—ë§Œ setupSession() í˜¸ì¶œ
        requestCameraPermission()
    }
    
    /// ì‚¬ìš©ìì—ê²Œ ì¹´ë©”ë¼ ê¶Œí•œì„ ìš”ì²­í•˜ê³ , í—ˆìš©ë˜ë©´ ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.
    private func requestCameraPermission() {
        AVCaptureDevice.requestAccess(for: .video) { [weak self] granted in
            guard granted else {
                print("âŒ ì¹´ë©”ë¼ ê¶Œí•œ ê±°ë¶€ë¨")
                return
            }
            DispatchQueue.main.async {
                self?.setupSession()
            }
        }
    }
    
    /// AVCaptureSession êµ¬ì„± ë° ì‹œì‘
    private func setupSession() {
        captureSession.beginConfiguration()
        captureSession.sessionPreset = .high
        
        // ì…ë ¥: í›„ë©´ ì¹´ë©”ë¼
        guard let device = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                   for: .video,
                                                   position: .back),
              let input = try? AVCaptureDeviceInput(device: device),
              captureSession.canAddInput(input) else {
            print("âŒ ì¹´ë©”ë¼ ì…ë ¥ ì„¤ì • ì‹¤íŒ¨")
            return
        }
        captureSession.addInput(input)
        
        // ì¶œë ¥: í”„ë ˆì„ ë¸ë¦¬ê²Œì´íŠ¸
        videoOutput.setSampleBufferDelegate(self,
                                            queue: DispatchQueue(label: "camera.queue"))
        videoOutput.alwaysDiscardsLateVideoFrames = true
        guard captureSession.canAddOutput(videoOutput) else {
            print("âŒ ë¹„ë””ì˜¤ ì¶œë ¥ ì¶”ê°€ ì‹¤íŒ¨")
            return
        }
        captureSession.addOutput(videoOutput)
        
        captureSession.commitConfiguration()
        captureSession.startRunning()
    }
    
    /// SwiftUIì—ì„œ PreviewLayerì— ì—°ê²°í•˜ê¸° ìœ„í•´ ì„¸ì…˜ ë°˜í™˜
    func getSession() -> AVCaptureSession {
        captureSession
    }
}

/// í™”ë©´ì— ë³´ì—¬ì¤„ ë‹¨ì¼ íƒì§€ ê²°ê³¼ ëª¨ë¸
struct DetectedObject: Identifiable {
    let id = UUID()
    let label: String
    let confidence: Float
    let boundingBox: CGRect    // 0~1ë¡œ ì •ê·œí™”ëœ ë°•ìŠ¤
}

extension CameraSessionManager: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput,
                       didOutput sampleBuffer: CMSampleBuffer,
                       from connection: AVCaptureConnection) {
        // 1) PixelBuffer ì¶”ì¶œ
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer),
              let model = visionModel else { return }
        
        // ìµœì‹  í”„ë ˆì„ ì €ì¥ (CameraViewì—ì„œ OCR ìš©ë„ë¡œ ì‚¬ìš©)
        currentBuffer = pixelBuffer
        
        // 2) Vision ìš”ì²­ ìƒì„±
        let request = VNCoreMLRequest(model: model) { [weak self] request, error in
            if let error = error {
                print("ğŸ›‘ VNCoreMLRequest ì—ëŸ¬:", error)
                return
            }
            guard let results = request.results as? [VNRecognizedObjectObservation] else { return }
            
            // 3) í•„í„°ë§: ì‹ ë¢°ë„ & ë©´ì 
            let minConf: VNConfidence = 0.9
            let minArea: CGFloat = 0.01  // ì •ê·œí™” ë©´ì  ê¸°ì¤€ (1%)
            
            let filtered = results.compactMap { obs -> DetectedObject? in
                guard
                    let top = obs.labels.first,
                    top.confidence >= minConf,
                    obs.boundingBox.width * obs.boundingBox.height >= minArea
                else {
                    return nil
                }
                
                return DetectedObject(
                    label: top.identifier,
                    confidence: top.confidence,
                    boundingBox: obs.boundingBox
                )
            }
            
            // 4) í¼ë¸”ë¦¬ì‹œ
            DispatchQueue.main.async {
                self?.detections = filtered
            }
        }
        request.imageCropAndScaleOption = .scaleFill
        
        // 5) í•¸ë“¤ëŸ¬ ìˆ˜í–‰ (.up ë°©í–¥)
        let handler = VNImageRequestHandler(
            cvPixelBuffer: pixelBuffer,
            orientation: .up,
            options: [:]
        )
        do {
            try handler.perform([request])
        } catch {
            print("ğŸ›‘ Vision perform ì—ëŸ¬:", error)
        }
    }
}
