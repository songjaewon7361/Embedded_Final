// ìƒëµëœ importëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
import SwiftUI
import Vision
import AVFoundation
import CoreImage

struct CameraView: View {
    // MARK: - ì˜ì¡´ ê°ì²´
    @StateObject private var sessionManager = CameraSessionManager()
    @StateObject private var tracker        = ImprovedByteTrackTracker()
    @StateObject private var speechSynthObj = SpeechSynthWrapper()

    // MARK: - ìƒíƒœ ê°’
    @State private var currentTracks: [Track] = []
    @State private var ocrTexts:     [Int: String] = [:]
    @State private var ocrAttempts:  [Int: Int]    = [:]
    @State private var debounce:     [Int: Int]    = [:]
    @State private var spokenTracks: Set<Int>      = []
    @State private var appearedFrames: [Int: Int]  = [:]
    @State private var frameCount = 0

    private let maxOCRAttempts  = 3
    private let retryInterval   = 15
    private let ciContext = CIContext(options: [.priorityRequestLow: true])

    init() {
        configureAudioSession()
    }

    // MARK: - ë°”ë””
    var body: some View {
        GeometryReader { geo in
            ZStack {
                // ì‹¤ì‹œê°„ ì¹´ë©”ë¼ í”„ë¦¬ë·°
                CameraPreview(session: sessionManager.getSession())
                    .ignoresSafeArea()

                // ë°”ìš´ë”©ë°•ìŠ¤ ë° ë¼ë²¨ ì˜¤ë²„ë ˆì´
                ForEach(currentTracks, id: \.id) { track in
                    overlay(for: track, in: geo)
                }
            }
            // Vision íƒì§€ ê²°ê³¼ ìˆ˜ì‹ 
            .onReceive(sessionManager.$detections) { detections in
                handleDetections(detections)
            }
        }
    }
}

// MARK: - ì˜¤ë””ì˜¤ ì„¸ì…˜
private extension CameraView {
    func configureAudioSession() {
        let session = AVAudioSession.sharedInstance()
        do {
            try session.setCategory(.playback,
                                    mode: .spokenAudio,
                                    options: [.duckOthers, .mixWithOthers])
            try session.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            print("ğŸ›‘ AudioSession ì„¤ì • ì‹¤íŒ¨:", error)
        }
    }
}

// MARK: - ì˜¤ë²„ë ˆì´ UI
private extension CameraView {
    @ViewBuilder
    func overlay(for track: Track, in geo: GeometryProxy) -> some View {
        // Vision â†’ í™”ë©´ ì¢Œí‘œ ë³€í™˜
        let rect = VNImageRectForNormalizedRect(track.boundingBox,
                                                Int(geo.size.width),
                                                Int(geo.size.height))

        // ë°”ìš´ë”© ë°•ìŠ¤
        Rectangle()
            .stroke(.green, lineWidth: 2)
            .frame(width: rect.width, height: rect.height)
            .position(x: rect.midX, y: rect.midY)

        // íŠ¸ë™ ID
        Text("ID \(track.id)")
            .font(.caption2).bold()
            .foregroundColor(.green)
            .padding(4)
            .background(Color.black.opacity(0.6))
            .position(x: rect.minX + 40, y: rect.minY + 12)

        // ê°ì²´ ë¶„ë¥˜ ë¼ë²¨ (Vision íƒì§€ ê²°ê³¼)
        if let det = detection(for: track) {
            Text("\(det.label) \(String(format: "%.2f", det.confidence))")
                .font(.caption2)
                .foregroundColor(.white)
                .padding(4)
                .background(Color.black.opacity(0.6))
                .position(x: rect.midX, y: rect.minY - 12)
        }

        // OCR ê²°ê³¼ / ë¼ë²¨ ì—†ìŒ í‘œì‹œ
        let tries = ocrAttempts[track.id] ?? 0
        if tries > 0 {
            let raw = ocrTexts[track.id] ?? ""
            let display = raw.isEmpty && tries >= maxOCRAttempts ? "ë¼ë²¨ ì—†ìŒ" : raw
            let color: Color = raw.isEmpty ? .blue : .yellow

            Text(display)
                .font(.caption2)
                .foregroundColor(color)
                .padding(4)
                .background(Color.black.opacity(0.6))
                .position(x: rect.midX, y: rect.maxY + 12)
        }
    }

    /// Vision íƒì§€ ê²°ê³¼ì™€ ByteTrack íŠ¸ë™ì„ ë§¤ì¹­
    func detection(for track: Track) -> DetectedObject? {
        sessionManager.detections.first {
            abs($0.boundingBox.midX - track.boundingBox.midX) < 0.01 &&
            abs($0.boundingBox.midY - track.boundingBox.midY) < 0.01
        }
    }
}

// MARK: - íƒì§€ ì²˜ë¦¬ & OCR / TTS
private extension CameraView {
    func handleDetections(_ detections: [DetectedObject]) {
        guard let buffer = sessionManager.currentBuffer else { return }

        frameCount += 1
        currentTracks = tracker.update(detections: detections)

        for track in currentTracks {
            let id        = track.id
            let lifetime  = frameCount - appearedFrames[id, default: frameCount]
            let hasText   = !(ocrTexts[id] ?? "").isEmpty
            let attempts  = ocrAttempts[id] ?? 0

            // ìµœì´ˆ ë“±ì¥ í”„ë ˆì„ ê¸°ë¡
            appearedFrames[id] = appearedFrames[id] ?? frameCount

            // ğŸ“¸ OCR ì‹œë„ (maxOCRAttempts íšŒê¹Œì§€, retryInterval í”„ë ˆì„ë§ˆë‹¤)
            if !hasText,
               attempts < maxOCRAttempts,
               frameCount % retryInterval == 0 {

                ocrAttempts[id] = attempts + 1

                performOCR(on: buffer, in: track.boundingBox) { text in
                    DispatchQueue.main.async {
                        let prev = ocrTexts[id] ?? ""
                        ocrTexts[id] = text
                        debounce[id] = (text == prev && !text.isEmpty)
                                       ? debounce[id, default: 0] + 1 : 1

                        // OCR ì§í›„ TTS (ë¼ë²¨ ìœ /ë¬´ ë‘˜ ë‹¤ ê°€ëŠ¥)
                        if !spokenTracks.contains(id), lifetime >= 60,
                           let msg = speakMessage(for: track, hasLabel: !text.isEmpty) {

                            print("ğŸ—£ï¸ OCR ì§í›„ TTS: \(msg)")
                            speechSynthObj.speak(msg)
                            spokenTracks.insert(id)
                        }
                    }
                }
            }

            // âœ… ë¼ë²¨ì´ ì´ë¯¸ ì¸ì‹ëœ ê²½ìš° â†’ TTS(ì¤‘ë³µ ë°©ì§€)
            if hasText,
               !spokenTracks.contains(id),
               lifetime >= 30,
               let msg = speakMessage(for: track, hasLabel: true) {

                print("ğŸ—£ï¸ OCR ì„±ê³µ í›„ TTS ì‹¤í–‰: \(msg)")
                speechSynthObj.speak(msg)
                spokenTracks.insert(id)
            }

            // âœ¨ **OCR ìµœì¢… ì‹¤íŒ¨ fallback** --------------------------
            // OCRì„ maxOCRAttempts ë²ˆ ì‹œë„í–ˆì§€ë§Œ ì—¬ì „íˆ í…ìŠ¤íŠ¸ê°€ ì—†ì„ ë•Œ,
            // TTSê°€ í•œ ë²ˆë„ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‹¤ë©´ ë¼ë²¨ ì—†ìŒ ë©”ì‹œì§€ë¥¼ ì¶œë ¥
            if !hasText,
               attempts >= maxOCRAttempts,
               !spokenTracks.contains(id),
               lifetime >= 30,
               let msg = speakMessage(for: track, hasLabel: false) {

                print("ğŸ—£ï¸ OCR ì‹¤íŒ¨ í›„ fallback TTS: \(msg)")
                speechSynthObj.speak(msg)
                spokenTracks.insert(id)
            }
            // ----------------------------------------------------
        }

        // ë©”ëª¨ë¦¬ ì •ë¦¬: ì‚¬ë¼ì§„ íŠ¸ë™ ì •ë³´ ì œê±°
        let live = Set(currentTracks.map(\.id))
        ocrTexts.keep(keys: live)
        ocrAttempts.keep(keys: live)
        debounce.keep(keys: live)
        spokenTracks = spokenTracks.intersection(live)
        appearedFrames = appearedFrames.filter { live.contains($0.key) }
    }
}

// MARK: - OCR
private extension CameraView {
    func performOCR(on buffer: CVPixelBuffer,
                    in normBox: CGRect,
                    completion: @escaping (String) -> Void) {

        let ciSrc = CIImage(cvPixelBuffer: buffer)
        let w = CGFloat(CVPixelBufferGetWidth(buffer))
        let h = CGFloat(CVPixelBufferGetHeight(buffer))

        // PixelBuffer ì¢Œí‘œê³„ â†’ crop ì˜ì—­
        var crop = CGRect(x: normBox.minX * w,
                          y: normBox.minY * h,
                          width: normBox.width * w,
                          height: normBox.height * h).integral
        crop = crop.intersection(CGRect(origin: .zero, size: CGSize(width: w, height: h)))
        guard !crop.isEmpty else { completion(""); return }

        // ëŒ€ë¹„ ì‚´ì§ ì¡°ì •
        let ciCrop = ciSrc.cropped(to: crop)
        let filt = CIFilter(name: "CIColorControls")!
        filt.setValue(ciCrop, forKey: kCIInputImageKey)
        filt.setValue(1.1, forKey: kCIInputContrastKey)
        let ciFinal = filt.outputImage ?? ciCrop

        guard let cg = ciContext.createCGImage(ciFinal, from: ciFinal.extent)
        else { completion(""); return }

        // í…ìŠ¤íŠ¸ ì¸ì‹ ìš”ì²­
        let req = VNRecognizeTextRequest { req, _ in
            let obs = (req.results as? [VNRecognizedTextObservation]) ?? []
            let txt = obs.first?.topCandidates(1).first?.string ?? ""
            completion(txt)
        }
        req.recognitionLevel       = .accurate
        req.usesLanguageCorrection = true
        req.recognitionLanguages   = ["ko-KR", "en-US"]

        DispatchQueue.global(qos: .userInitiated).async {
            try? VNImageRequestHandler(cgImage: cg, options: [:]).perform([req])
        }
    }
}

// MARK: - TTS ë©”ì‹œì§€ ìƒì„±
private extension CameraView {
    /// `hasLabel` â€“ OCRë¡œ ë¼ë²¨(í…ìŠ¤íŠ¸)ì´ ê²€ì¶œë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
    func speakMessage(for track: Track, hasLabel: Bool) -> String? {
        guard let det = detection(for: track) else { return nil }

        if det.label == "íˆ¬ëª…_pet" {
            return hasLabel
                ? "íˆ¬ëª… í˜íŠ¸ë³‘ì…ë‹ˆë‹¤. ë¼ë²¨ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¼ë²¨ì„ ì œê±°í•œ í›„, íˆ¬ëª… í˜íŠ¸ë³‘ ì „ìš© ìˆ˜ê±°í•¨ì— ë¶„ë¦¬í•´ ì£¼ì„¸ìš”."
                : "íˆ¬ëª… í˜íŠ¸ë³‘ì…ë‹ˆë‹¤. íˆ¬ëª… í˜íŠ¸ë³‘ ì „ìš© ìˆ˜ê±°í•¨ì— ë¶„ë¦¬ìˆ˜ê±°í•´ ì£¼ì„¸ìš”."
        } else {
            return hasLabel
                ? "ìœ ìƒ‰ í”Œë¼ìŠ¤í‹±ì…ë‹ˆë‹¤. ë¼ë²¨ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¼ë²¨ì„ ì œê±°í•œ í›„, ì¼ë°˜ í”Œë¼ìŠ¤í‹± ì „ìš© ìˆ˜ê±°í•¨ì— ë¶„ë¦¬í•´ ì£¼ì„¸ìš”."
                : "ìœ ìƒ‰ í”Œë¼ìŠ¤í‹±ì…ë‹ˆë‹¤. ì¼ë°˜ í”Œë¼ìŠ¤í‹± ìˆ˜ê±°í•¨ì— ë¶„ë¦¬ìˆ˜ê±°í•´ ì£¼ì„¸ìš”."
        }
    }
}

// MARK: - TTS ë˜í¼
final class SpeechSynthWrapper: ObservableObject {
    private let synthesizer = AVSpeechSynthesizer()

    /// ì¤‘ë³µ ë°œí™” ë°©ì§€ë¥¼ ìœ„í•´ `isSpeaking` ê²€ì‚¬ë¥¼ í¬í•¨
    func speak(_ msg: String) {
        DispatchQueue.main.async {
            if self.synthesizer.isSpeaking {
                print("â›”ï¸ ì¤‘ë³µ ë°œí™” ë°©ì§€: ì´ë¯¸ TTS ì§„í–‰ ì¤‘")
                return
            }
            print("ğŸ”Š ìŒì„± ì¶œë ¥: \(msg)")
            let utt = AVSpeechUtterance(string: msg)
            utt.voice = AVSpeechSynthesisVoice(language: "ko-KR")
            self.synthesizer.speak(utt)
        }
    }
}

// MARK: - Dictionary í™•ì¥
private extension Dictionary where Key == Int {
    /// ì£¼ì–´ì§„ í‚¤ ì§‘í•©ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°
    mutating func keep(keys: Set<Int>) {
        self = self.filter { keys.contains($0.key) }
    }
}
